{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Functions (first draft)\n",
    "## Oct. 19, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using code developed in file from Oct. 18, 2021 to develop scraping functions for minor league data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearly_links(year, teams_excl = []):\n",
    "    '''\n",
    "    Takes as input a year and teams, and returns a collection of links to minor league teams for that year\n",
    "    \n",
    "    Input:\n",
    "    year, a float corresponding to the year\n",
    "    teams, a list containing the indices of teams to EXCLUDE, where:\n",
    "    0 = MLB\n",
    "    1 = Triple-A\n",
    "    2 = Double-A\n",
    "    3 = High-A\n",
    "    4 = Low-A\n",
    "    5 = Short-Season A\n",
    "    6 = Rookie\n",
    "    7 = Foreign Rookie\n",
    "    \n",
    "    Output:\n",
    "    links, a list of length 30 of lists of length 8 of lists of teams at that level (index 0 = MLB team)\n",
    "    '''\n",
    "    # Creating BeautifulSoup object from URL\n",
    "    \n",
    "    url = f'https://www.baseball-reference.com/register/affiliate.cgi?year={year}' # URL to use\n",
    "    html = urlopen(url) # collecting HTML data\n",
    "    soup = BeautifulSoup(html, features=\"lxml\") # creating object from HTML\n",
    "\n",
    "    # Gets non-header rows and declares blank list to add links to team pages\n",
    "    \n",
    "    rows = soup.findAll('tr')[1:] # excluding the header row\n",
    "    links = []\n",
    "\n",
    "    # Iterates over rows, gets cell in row, gets links to teams in that row (if they exist)\n",
    "    # Includes MLB team as well, assuming they're in the list of teams to scrape\n",
    "    \n",
    "    for i in range(len(rows)):\n",
    "        \n",
    "        row = rows[i]\n",
    "        row_data = [] # data for each row\n",
    "                    \n",
    "        # Gets link to franchise page, and extracts abbreviation and adds to data\n",
    "            \n",
    "        franchise_link = row.findAll('th')[0].a['href']\n",
    "        pattern = '\\=(.*?)\\&'\n",
    "        franchise = re.search(pattern, franchise_link).group(1)\n",
    "            \n",
    "        # Baseball Reference hasn't adjusted Expos/Nationals abbreviations\n",
    "        # Changing this manually when necessary\n",
    "            \n",
    "        if franchise == 'WSN' and year < 2005:\n",
    "            franchise = 'MON'\n",
    "            \n",
    "        row_data.append([f'/teams/{franchise}/{year}.shtml']) # adds as its own list\n",
    "            \n",
    "        elements = row.findAll('td') # all elements corresponding to affiliates\n",
    "        \n",
    "        for td in elements: # for each affiliate level\n",
    "            \n",
    "            affils = td.findAll('a') # links to all possible affiliates\n",
    "            affil_list = []\n",
    "            \n",
    "            if len(affils) != 0: # if the team has an affiliate at this level\n",
    "                \n",
    "                for affil in affils:\n",
    "                    affil_list.append(affil['href'])\n",
    "                    \n",
    "                row_data.append(affil_list)\n",
    "                \n",
    "            else:\n",
    "                row_data.append(['']) # adds a blank list for levels without an affiliate\n",
    "                \n",
    "        links.append(row_data)\n",
    "        \n",
    "    # The second column just counts the number of teams for each organization\n",
    "    # It doesn't have an HTML link, so we just exclude these elements from our lists\n",
    "    # This also excludes whatever teams we input into the function\n",
    "        \n",
    "    for elem in links:\n",
    "        elem.pop(1)\n",
    "        for i in sorted(teams_excl, reverse=True):\n",
    "            del elem[i]\n",
    "                \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_data(url_ext):\n",
    "    '''\n",
    "    Takes as input a URL extension, and returns a collection of links to minor league teams for that year\n",
    "    \n",
    "    Input:\n",
    "    url_ext, a string that will be appended to the Baseball Reference page and scraped\n",
    "    \n",
    "    Output:\n",
    "    team_df_bat, a dataframe containing batting information for each player on that team\n",
    "    team_df_pitch, a dataframe containing pitching information for each player on that team\n",
    "    '''\n",
    "    # Creating BeautifulSoup object from URL\n",
    "    \n",
    "    page = requests.get(f'https://www.baseball-reference.com{url_ext}').text # URL to use\n",
    "\n",
    "    # Gets subset of text corresponding to the two tables of interest\n",
    "    # Doing it this way becuase some info is contained in HTML comments\n",
    "    \n",
    "    batting = page[page.find('<table class=\"sortable stats_table\" id=\"team_batting\"'):\n",
    "                   page.find('<div class=\"footer no_hide_long\" id=\"tfooter_team_batting\"')]\n",
    "    \n",
    "    pitching = page[page.find('<table class=\"sortable stats_table\" id=\"team_pitching\"'):\n",
    "                    page.find('<div class=\"footer no_hide_long\" id=\"tfooter_team_pitching\"')]\n",
    "    \n",
    "    # Creating BeautifulSoup objects\n",
    "    \n",
    "    soup_bat = BeautifulSoup(batting, features=\"lxml\")\n",
    "    soup_pitch = BeautifulSoup(pitching, features=\"lxml\")\n",
    "    \n",
    "    # Getting list of headers for each table, excluding ordering column \n",
    "    # Adds in var. for whether player played in majors\n",
    "    \n",
    "    headers_bat = [th.getText() for th in soup_bat.findAll('tr', limit=2)[0].findAll('th')][1:]\n",
    "    headers_bat.append('MLB')\n",
    "    headers_pitch = [th.getText() for th in soup_pitch.findAll('tr', limit=2)[0].findAll('th')][1:]\n",
    "    headers_pitch.append('MLB')\n",
    "    \n",
    "    # Getting rows for each table\n",
    "\n",
    "    bat_rows = soup_bat.findAll('tr')[1:] # excluding the header row, but including the summary row\n",
    "    pitch_rows = soup_pitch.findAll('tr')[1:]\n",
    "\n",
    "    # Putting them into their own tables\n",
    "    \n",
    "    team_bat = []\n",
    "    team_pitch = []\n",
    "    \n",
    "    # Iterates over row and adds text from table elements\n",
    "\n",
    "    for i in range(len(bat_rows)):\n",
    "        row_data = []\n",
    "        row_text = str(bat_rows[i])\n",
    "\n",
    "        for td in bat_rows[i].findAll('td'):\n",
    "            row_data.append(td.getText())\n",
    "\n",
    "        # Adds in whether the player played in the majors\n",
    "        # For minor league teams: this is when their name is bolded\n",
    "        # For major league teams: everyone, so the URL contains the string 'teams'\n",
    "        \n",
    "        if ('strong' in row_text) or ('teams' in url_ext):\n",
    "            row_data.append('1')\n",
    "        else:\n",
    "            row_data.append('0')\n",
    "\n",
    "        team_bat.append(row_data)\n",
    "\n",
    "    # Replicates the loop above, but for pitching data\n",
    "\n",
    "    for i in range(len(pitch_rows)):\n",
    "        row_data = []\n",
    "        row_text = str(pitch_rows[i])\n",
    "\n",
    "        for td in pitch_rows[i].findAll('td'):\n",
    "            row_data.append(td.getText())\n",
    "\n",
    "        if ('strong' in row_text) or ('teams' in url_ext):\n",
    "            row_data.append('1')\n",
    "        else:\n",
    "            row_data.append('0')\n",
    "\n",
    "        team_pitch.append(row_data)\n",
    "        \n",
    "    # Converting these to dataframes and returning\n",
    "    \n",
    "    bat = pd.DataFrame(team_bat, columns=headers_bat)\n",
    "    pitch = pd.DataFrame(team_pitch, columns=headers_pitch)\n",
    "    \n",
    "    return bat, pitch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
